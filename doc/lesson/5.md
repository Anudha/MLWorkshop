GOALS:  
# Discuss Natural Language Processing
# Implement a model from HuggingFace (T5-small) should work for on most computers with CPU
# Qualitatively try out a few phrases
# See that performance metrics, especially for generative AI, require work
# Review data cleaning method for text data 

# code
from transformers import pipeline
paraphrase_pipeline = pipeline(“text2text-generation”, model=”t5-small”, tokenizer=”t5-small”)
# end of code

Reference: https://jmlr.org/papers/volume21/20-074/20-074.pdf
t5 = text-to-text transfer transformer

Dataset(s)
“Colossal Clean Crawled Corpus” (C4),100s GB of clean English text scraped from the web

This paper outlines their data cleaning steps.  Do these data pre-processing steps apply in another business use-case?

“

• We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark).
applicable for cleaning raw chat or blog data
applicable when volume of data >> needed

• We discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words.
applicable for extracting text from long reports / documents where the goal is to process text (although graphs, diagrams contain important info)

• We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”

applicable for user forums especially
doesn’t matter for company reports

Crude ways are used as they are easy to implement.  
There are lists for a) STOP words, b) emotionally negative words.  
Implement these as filters in your work depending on source of content. 
https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words

• Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript. • Some pages had placeholder “lorem ipsum” text; we removed any page where the phrase “lorem ipsum” appeared.

applicable for scraping code
example of domain specific or application specific data cleaning

• Some pages inadvertently contained code. Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket.

applicable for scraping code
example of domain specific or application specific data cleaning

• To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set.”

delete duplications. is the duplication relevant due to related context?
delete semantically meaningful repeated content (although paraphrased?)
Evaluation / testing of clean data

# Where does this lesson build on a previous discussion?
- Clustering sentences, and then choosing one from each cluster, is a method for deduplication.
- Which previous lesson discusses clustering algorithms?  Clustering is one of the most important ideas in unsupervised learning.
- Ideally we should do data scraping via beautiful soup (or another lib).  As data scraping is a key skill.
- Even when organizations have their own data, they may want to compare or augment with public data or take advantage of content on a competitor's site.

